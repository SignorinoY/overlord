@article{corinzia_variational_2021,
  title    = {Variational {Federated} {Multi}-{Task} {Learning}},
  url      = {http://arxiv.org/abs/1906.06268},
  abstract = {In federated learning, a central server coordinates the training of a single model on a massively distributed network of devices. This setting can be naturally extended to a multi-task learning framework, to handle real-world federated datasets that typically show strong statistical heterogeneity among devices. Despite federated multi-task learning being shown to be an effective paradigm for real-world datasets, it has been applied only on convex models. In this work, we introduce VIRTUAL, an algorithm for federated multi-task learning for general non-convex models. In VIRTUAL the federated network of the server and the clients is treated as a star-shaped Bayesian network, and learning is performed on the network using approximated variational inference. We show that this method is effective on real-world federated datasets, outperforming the current state-of-the-art for federated learning, and concurrently allowing sparser gradient updates.},
  urldate  = {2021-05-08},
  journal  = {arXiv:1906.06268 [cs, stat]},
  author   = {Corinzia, Luca and Beuret, Ami and Buhmann, Joachim M.},
  month    = feb,
  year     = {2021},
  note     = {arXiv: 1906.06268},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{eichner_semi-cyclic_2019,
  title     = {Semi-{Cyclic} {Stochastic} {Gradient} {Descent}},
  url       = {http://proceedings.mlr.press/v97/eichner19a.html},
  abstract  = {We consider convex SGD updates with a block-cyclic structure, i.e., where each cycle consists of a small number of blocks, each with many samples from a possibly different, block-specific, distribu...},
  language  = {en},
  urldate   = {2021-05-08},
  booktitle = {International {Conference} on {Machine} {Learning}},
  publisher = {PMLR},
  author    = {Eichner, Hubert and Koren, Tomer and Mcmahan, Brendan and Srebro, Nathan and Talwar, Kunal},
  month     = may,
  year      = {2019},
  note      = {ISSN: 2640-3498},
  pages     = {1764--1773}
}

@article{garcia-gonzalez_public_2020,
  title    = {A {Public} {Domain} {Dataset} for {Real}-{Life} {Human} {Activity} {Recognition} {Using} {Smartphone} {Sensors}},
  volume   = {20},
  issn     = {1424-8220},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7218897/},
  doi      = {10.3390/s20082200},
  abstract = {In recent years, human activity recognition has become a hot topic inside the scientific community. The reason to be under the spotlight is its direct application in multiple domains, like healthcare or fitness. Additionally, the current worldwide use of smartphones makes it particularly easy to get this kind of data from people in a non-intrusive and cheaper way, without the need for other wearables. In this paper, we introduce our orientation-independent, placement-independent and subject-independent human activity recognition dataset. The information in this dataset is the measurements from the accelerometer, gyroscope, magnetometer, and GPS of the smartphone. Additionally, each measure is associated with one of the four possible registered activities: inactive, active, walking and driving. This work also proposes asupport vector machine (SVM) model to perform some preliminary experiments on the dataset. Considering that this dataset was taken from smartphones in their actual use, unlike other datasets, the development of a good model on such data is an open problem and a challenge for researchers. By doing so, we would be able to close the gap between the model and a real-life application.},
  number   = {8},
  urldate  = {2021-05-11},
  journal  = {Sensors (Basel, Switzerland)},
  author   = {Garcia-Gonzalez, Daniel and Rivero, Daniel and Fernandez-Blanco, Enrique and Luaces, Miguel R.},
  month    = apr,
  year     = {2020},
  pmid     = {32295028},
  pmcid    = {PMC7218897}
}

@misc{gingsmith_gingsmithfmtl_2021,
  title     = {gingsmith/fmtl},
  copyright = {Apache-2.0 License         ,                 Apache-2.0 License},
  url       = {https://github.com/gingsmith/fmtl},
  abstract  = {Federated Multi-Task Learning. Contribute to gingsmith/fmtl development by creating an account on GitHub.},
  urldate   = {2021-05-11},
  author    = {gingsmith},
  month     = may,
  year      = {2021},
  note      = {original-date: 2017-10-20T21:10:51Z}
}

@misc{jadhav_ashwinrjfederated-learning-pytorch_2021,
  title     = {{AshwinRJ}/{Federated}-{Learning}-{PyTorch}},
  copyright = {MIT License         ,                 MIT License},
  url       = {https://github.com/AshwinRJ/Federated-Learning-PyTorch},
  abstract  = {Implementation of Communication-Efficient Learning of Deep Networks from Decentralized Data},
  urldate   = {2021-05-11},
  author    = {Jadhav, Ashwin R.},
  month     = may,
  year      = {2021},
  note      = {original-date: 2018-11-16T23:51:14Z},
  keywords  = {deep-learning, distributed-computing, federated-learning, python, pytorch}
}

@article{kairouz_advances_2021,
  title    = {Advances and {Open} {Problems} in {Federated} {Learning}},
  url      = {http://arxiv.org/abs/1912.04977},
  abstract = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
  urldate  = {2021-05-09},
  journal  = {arXiv:1912.04977 [cs, stat]},
  author   = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aurélien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D'Oliveira, Rafael G. L. and Eichner, Hubert and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gascón, Adrià and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Konečný, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancrède and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and Özgür, Ayfer and Pagh, Rasmus and Raykova, Mariana and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tramèr, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
  month    = mar,
  year     = {2021},
  note     = {arXiv: 1912.04977},
  keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{khodak_adaptive_2019,
  title    = {Adaptive {Gradient}-{Based} {Meta}-{Learning} {Methods}},
  url      = {http://arxiv.org/abs/1906.02717},
  abstract = {We build a theoretical framework for designing and understanding practical meta-learning methods that integrates sophisticated formalizations of task-similarity with the extensive literature on online convex optimization and sequential prediction algorithms. Our approach enables the task-similarity to be learned adaptively, provides sharper transfer-risk bounds in the setting of statistical learning-to-learn, and leads to straightforward derivations of average-case regret bounds for efficient algorithms in settings where the task-environment changes dynamically or the tasks share a certain geometric structure. We use our theory to modify several popular meta-learning algorithms and improve their meta-test-time performance on standard problems in few-shot learning and federated learning.},
  urldate  = {2021-05-08},
  journal  = {arXiv:1906.02717 [cs, stat]},
  author   = {Khodak, Mikhail and Balcan, Maria-Florina and Talwalkar, Ameet},
  month    = dec,
  year     = {2019},
  note     = {arXiv: 1906.02717},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence}
}

@article{kulkarni_survey_2020,
  title    = {Survey of {Personalization} {Techniques} for {Federated} {Learning}},
  url      = {http://arxiv.org/abs/2003.08673},
  abstract = {Federated learning enables machine learning models to learn from private decentralized data without compromising privacy. The standard formulation of federated learning produces one shared model for all clients. Statistical heterogeneity due to non-IID distribution of data across devices often leads to scenarios where, for some clients, the local models trained solely on their private data perform better than the global shared model thus taking away their incentive to participate in the process. Several techniques have been proposed to personalize global models to work better for individual clients. This paper highlights the need for personalization and surveys recent research on this topic.},
  urldate  = {2021-05-09},
  journal  = {arXiv:2003.08673 [cs, stat]},
  author   = {Kulkarni, Viraj and Kulkarni, Milind and Pant, Aniruddha},
  month    = mar,
  year     = {2020},
  note     = {arXiv: 2003.08673},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{lecun_mnist_2010,
  title   = {{MNIST} handwritten digit database},
  volume  = {2},
  journal = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  author  = {LeCun, Yann and Cortes, Corinna and Burges, CJ},
  year    = {2010}
}

@article{li_convergence_2020,
  title    = {On the {Convergence} of {FedAvg} on {Non}-{IID} {Data}},
  url      = {http://arxiv.org/abs/1907.02189},
  abstract = {Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging ({\textbackslash}texttt\{FedAvg\}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of {\textbackslash}texttt\{FedAvg\} on non-iid data and establish a convergence rate of \${\textbackslash}mathcal\{O\}({\textbackslash}frac\{1\}\{T\})\$ for strongly convex and smooth problems, where \$T\$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user devices may be disconnected from the server, we relax the assumption of full device participation to partial device participation and study different averaging schemes; low device participation rate can be achieved without severely slowing down the learning. Our results indicate that heterogeneity of data slows down the convergence, which matches empirical observations. Furthermore, we provide a necessary condition for {\textbackslash}texttt\{FedAvg\} on non-iid data: the learning rate \${\textbackslash}eta\$ must decay, even if full-gradient is used; otherwise, the solution will be \${\textbackslash}Omega ({\textbackslash}eta)\$ away from the optimal.},
  urldate  = {2021-05-10},
  journal  = {arXiv:1907.02189 [cs, math, stat]},
  author   = {Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  month    = jun,
  year     = {2020},
  note     = {arXiv: 1907.02189 version: 3},
  keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning}
}

@article{li_federated_2020,
  title      = {Federated {Learning}: {Challenges}, {Methods}, and {Future} {Directions}},
  volume     = {37},
  issn       = {1053-5888, 1558-0792},
  shorttitle = {Federated {Learning}},
  url        = {https://ieeexplore.ieee.org/document/9084352/},
  doi        = {10.1109/MSP.2020.2975749},
  abstract   = {Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.},
  language   = {en},
  number     = {3},
  urldate    = {2021-03-28},
  journal    = {IEEE Signal Processing Magazine},
  author     = {Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  month      = may,
  year       = {2020},
  pages      = {50--60}
}

@article{mcmahan_communication-efficient_2017,
  title    = {Communication-{Efficient} {Learning} of {Deep} {Networks} from {Decentralized} {Data}},
  url      = {http://arxiv.org/abs/1602.05629},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  urldate  = {2021-05-08},
  journal  = {arXiv:1602.05629 [cs]},
  author   = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Agüera y},
  month    = feb,
  year     = {2017},
  note     = {arXiv: 1602.05629},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{smith_federated_2017,
  address   = {Red Hook, NY, USA},
  series    = {{NIPS}'17},
  title     = {Federated multi-task learning},
  isbn      = {978-1-5108-6096-4},
  abstract  = {Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.},
  urldate   = {2021-05-07},
  booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates Inc.},
  author    = {Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet},
  month     = dec,
  year      = {2017},
  pages     = {4427--4437}
}

@inproceedings{zhang_convex_2010,
  address   = {Arlington, Virginia, USA},
  series    = {{UAI}'10},
  title     = {A convex formulation for learning task relationships in multi-task learning},
  isbn      = {978-0-9749039-6-5},
  abstract  = {Multi-task learning is a learning paradigm which seeks to improve the generalization performance of a learning task with the help of some other related tasks. In this paper, we propose a regularization formulation for learning the relationships between tasks in multi-task learning. This formulation can be viewed as a novel generalization of the regularization framework for single-task learning. Besides modeling positive task correlation, our method, called multi-task relationship learning (MTRL), can also describe negative task correlation and identify outlier tasks based on the same underlying principle. Under this regularization framework, the objective function of MTRL is convex. For efficiency, we use an alternating method to learn the optimal model parameters for each task as well as the relationships between tasks. We study MTRL in the symmetric multi-task learning setting and then generalize it to the asymmetric setting as well. We also study the relationships between MTRL and some existing multi-task learning methods. Experiments conducted on a toy problem as well as several benchmark data sets demonstrate the effectiveness of MTRL.},
  urldate   = {2021-05-10},
  booktitle = {Proceedings of the {Twenty}-{Sixth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
  publisher = {AUAI Press},
  author    = {Zhang, Yu and Yeung, Dit-Yan},
  month     = jul,
  year      = {2010},
  pages     = {733--742}
}

@article{zhao_federated_2018,
  title    = {Federated {Learning} with {Non}-{IID} {Data}},
  url      = {http://arxiv.org/abs/1806.00582},
  abstract = {Federated learning enables resource-constrained edge compute devices, such as mobile phones and IoT devices, to learn a shared model for prediction, while keeping the training data local. This decentralized approach to train models provides privacy, security, regulatory and economic benefits. In this work, we focus on the statistical challenge of federated learning when local data is non-IID. We first show that the accuracy of federated learning reduces significantly, by up to 55\% for neural networks trained for highly skewed non-IID data, where each client device trains only on a single class of data. We further show that this accuracy reduction can be explained by the weight divergence, which can be quantified by the earth mover's distance (EMD) between the distribution over classes on each device and the population distribution. As a solution, we propose a strategy to improve training on non-IID data by creating a small subset of data which is globally shared between all the edge devices. Experiments show that accuracy can be increased by 30\% for the CIFAR-10 dataset with only 5\% globally shared data.},
  urldate  = {2021-05-08},
  journal  = {arXiv:1806.00582 [cs, stat]},
  author   = {Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  month    = jun,
  year     = {2018},
  note     = {arXiv: 1806.00582},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}
